{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMYIPMaCGSd9/N2e0rlrUX0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlirezaParchami/Real-time-Character-Detection-using-Faster-RCNN/blob/main/train_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sxdzoKO30XZ"
      },
      "source": [
        "Install Dependencies and Detectron2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIvpKFRj3WLz"
      },
      "source": [
        "!pip install pyyaml==5.1\r\n",
        "import torch, torchvision\r\n",
        "print(torch.__version__, torch.cuda.is_available())\r\n",
        "!gcc --version\r\n",
        "\r\n",
        "import torch\r\n",
        "assert torch.__version__.startswith(\"1.7\")\r\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.7/index.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRBQXX9v3xRA"
      },
      "source": [
        "import detectron2\r\n",
        "from detectron2.utils.logger import setup_logger\r\n",
        "setup_logger()\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import os, json, cv2, random\r\n",
        "from google.colab.patches import cv2_imshow\r\n",
        "\r\n",
        "from detectron2.engine import DefaultPredictor\r\n",
        "from detectron2.config import get_cfg"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPi788WY5aSL"
      },
      "source": [
        "#Download Character Dataset and unzip\r\n",
        "#!wget https://github.com/MinhasKamal/AlphabetRecognizer/files/1084725/English.Alphabet.Dataset.zip\r\n",
        "#!unzip English.Alphabet.Dataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "iDsifkh6bPnM",
        "outputId": "24ea1fac-2799-4761-d372-3dd4a3fa0473"
      },
      "source": [
        "from detectron2.structures import BoxMode\r\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\r\n",
        "\r\n",
        "def get_character_dict(img_dir, num):\r\n",
        "  dir = img_dir + str(num)+ \"/\" + str(num) + \".json\"\r\n",
        "  json_file = os.path.join(dir)\r\n",
        "  \r\n",
        "  with open(json_file) as f:\r\n",
        "    imgs_anns = json.load(f)\r\n",
        "  \r\n",
        "  character_dicts = []\r\n",
        "  for idx, v in zip(imgs_anns.keys(), imgs_anns.values()):\r\n",
        "    record = {}\r\n",
        "\r\n",
        "    filename = os.path.join(img_dir + str(num) + \"/\" + v[\"filename\"])\r\n",
        "    height, width = cv2.imread(filename).shape[:2]\r\n",
        "\r\n",
        "    record[\"file_name\"] = filename\r\n",
        "    record[\"image_id\"] = idx\r\n",
        "    record[\"height\"] = height\r\n",
        "    record[\"width\"] = width\r\n",
        "\r\n",
        "    annos = v[\"regions\"]\r\n",
        "    objs = []\r\n",
        "    for _, anno in annos.items():\r\n",
        "      anno = anno[\"shape_attributes\"]\r\n",
        "      px = anno[\"points_x\"]\r\n",
        "      py = anno[\"points_y\"]\r\n",
        "      poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\r\n",
        "      poly = [p for x in poly for p in x]\r\n",
        "\r\n",
        "      obj = {\r\n",
        "          \"bbox\": [np.min(px), np.min(py), np.max(px), np.max(py)],\r\n",
        "          \"bbox_mode\": BoxMode.XYXY_ABS,\r\n",
        "          \"segmentation\": [poly],\r\n",
        "          \"category_id\": num - 1,\r\n",
        "      }\r\n",
        "      objs.append(obj)\r\n",
        "    record[\"annotations\"] = objs\r\n",
        "    character_dicts.append(record)\r\n",
        "  return character_dicts\r\n",
        "  \r\n",
        "\r\n",
        "for d in range(1,8):\r\n",
        "  DatasetCatalog.register(\"character_train_\" + str(d), lambda d=d: get_balloon_dicts(\"English Alphabet Dataset/\", d))\r\n",
        "  MetadataCatalog.get(\"character_train_\" + str(d)).set(thing_classes=[str(d)])\r\n",
        "character_metadata = MetadataCatalog.get(\"character_train\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Metadata(name='character_train')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-0e44ad0b2585>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mcharacter_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMetadataCatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"character_train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcharacter_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcharacter_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'Metadata' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKoLgcuBRkjK"
      },
      "source": [
        "#Check if the metadata is added successfully\r\n",
        "\r\n",
        "from detectron2.utils.visualizer import Visualizer\r\n",
        "dataset_dicts = get_character_dict(\"English Alphabet Dataset/\", 5)\r\n",
        "print(len(dataset_dicts))\r\n",
        "print(len(dataset_dicts[1]))\r\n",
        "print(dataset_dicts[1])\r\n",
        "for d in random.sample(dataset_dicts, 3):\r\n",
        "  print(d)\r\n",
        "  img = cv2.imread(d[\"file_name\"])\r\n",
        "  visualizer = Visualizer(img[:, :, ::-1], metadata=character_metadata, scale=0.5)\r\n",
        "  out = visualizer.draw_dataset_dict(d)\r\n",
        "  cv2_imshow(out.get_image()[:, :, ::-1])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}