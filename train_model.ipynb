{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOFjIWvtmzvTRBt63V5QCuz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlirezaParchami/Real-time-Character-Detection-using-Faster-RCNN/blob/main/train_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sxdzoKO30XZ"
      },
      "source": [
        "Install Dependencies and Detectron2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIvpKFRj3WLz"
      },
      "source": [
        "!pip install pyyaml==5.1\r\n",
        "import torch, torchvision\r\n",
        "print(torch.__version__, torch.cuda.is_available())\r\n",
        "!gcc --version\r\n",
        "\r\n",
        "import torch\r\n",
        "assert torch.__version__.startswith(\"1.7\")\r\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.7/index.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRBQXX9v3xRA"
      },
      "source": [
        "import detectron2\r\n",
        "from detectron2.utils.logger import setup_logger\r\n",
        "setup_logger()\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import os, json, cv2, random\r\n",
        "from google.colab.patches import cv2_imshow\r\n",
        "\r\n",
        "from detectron2.engine import DefaultPredictor\r\n",
        "from detectron2.config import get_cfg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPi788WY5aSL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89cf1296-ebaa-4dc6-cc3c-c880fd16ec62"
      },
      "source": [
        "#Download Character Dataset and unzip\r\n",
        "#!wget https://github.com/MinhasKamal/AlphabetRecognizer/files/1084725/English.Alphabet.Dataset.zip\r\n",
        "!unzip English.Alphabet.Dataset.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  English.Alphabet.Dataset.zip\n",
            "replace English Alphabet Dataset/test/1/1.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace English Alphabet Dataset/test/1/1_1.JPG? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDsifkh6bPnM",
        "outputId": "232e8ee4-4f61-49b0-c077-7b193a0c3961"
      },
      "source": [
        "# nakoooooooooooooooooooooooooooooooooooooooon\r\n",
        "#Version 1\r\n",
        "from detectron2.structures import BoxMode\r\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\r\n",
        "\r\n",
        "def get_character_dict(img_dir, num):\r\n",
        "  dir = img_dir + str(num)+ \"/\" + str(num) + \".json\"\r\n",
        "  json_file = os.path.join(dir)\r\n",
        "  \r\n",
        "  with open(json_file) as f:\r\n",
        "    imgs_anns = json.load(f)\r\n",
        "  \r\n",
        "  character_dicts = []\r\n",
        "  for idx, v in zip(imgs_anns.keys(), imgs_anns.values()):\r\n",
        "    record = {}\r\n",
        "\r\n",
        "    filename = os.path.join(img_dir + str(num) + \"/\" + v[\"filename\"])\r\n",
        "    height, width = cv2.imread(filename).shape[:2]\r\n",
        "\r\n",
        "    record[\"file_name\"] = filename\r\n",
        "    record[\"image_id\"] = idx\r\n",
        "    record[\"height\"] = height\r\n",
        "    record[\"width\"] = width\r\n",
        "\r\n",
        "    annos = v[\"regions\"]\r\n",
        "    objs = []\r\n",
        "    for _, anno in annos.items():\r\n",
        "      anno = anno[\"shape_attributes\"]\r\n",
        "      px = anno[\"points_x\"]\r\n",
        "      py = anno[\"points_y\"]\r\n",
        "      poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\r\n",
        "      poly = [p for x in poly for p in x]\r\n",
        "\r\n",
        "      obj = {\r\n",
        "          \"bbox\": [np.min(px), np.min(py), np.max(px), np.max(py)],\r\n",
        "          \"bbox_mode\": BoxMode.XYXY_ABS,\r\n",
        "          \"segmentation\": [poly],\r\n",
        "          \"category_id\": num - 1,\r\n",
        "      }\r\n",
        "      objs.append(obj)\r\n",
        "    record[\"annotations\"] = objs\r\n",
        "    character_dicts.append(record)\r\n",
        "  return character_dicts\r\n",
        "  \r\n",
        "\r\n",
        "for d in range(1,8):\r\n",
        "  DatasetCatalog.register(\"character_train_\" + str(d), lambda d=d: get_character_dict(\"English Alphabet Dataset/\", d))\r\n",
        "  MetadataCatalog.get(\"character_train_\" + str(d)).set(thing_classes=[str(d)])\r\n",
        "character_metadata = MetadataCatalog.get(\"character_train\")\r\n",
        "print(\"Finished Successfully\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished Successfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "7wKxIw7sbwQ_",
        "outputId": "0951a8d6-b281-44dc-c302-14d6837daef8"
      },
      "source": [
        "#Version 2\r\n",
        "from detectron2.structures import BoxMode\r\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\r\n",
        "\r\n",
        "def get_character_dict(img_dir):\r\n",
        "  dataset_dicts = []\r\n",
        "  for num in range(1,8):\r\n",
        "    dir = img_dir + str(num)+ \"/\" + str(num) + \".json\"\r\n",
        "    json_file = os.path.join(dir)\r\n",
        "  \r\n",
        "    with open(json_file) as f:\r\n",
        "      imgs_anns = json.load(f)\r\n",
        "  \r\n",
        "    character_dicts = []\r\n",
        "    for idx, v in zip(imgs_anns.keys(), imgs_anns.values()):\r\n",
        "      record = {}\r\n",
        "\r\n",
        "      filename = os.path.join(img_dir + str(num) + \"/\" + v[\"filename\"])\r\n",
        "      height, width = cv2.imread(filename).shape[:2]\r\n",
        "\r\n",
        "      record[\"file_name\"] = filename\r\n",
        "      record[\"image_id\"] = idx\r\n",
        "      record[\"height\"] = height\r\n",
        "      record[\"width\"] = width\r\n",
        "\r\n",
        "      annos = v[\"regions\"]\r\n",
        "      objs = []\r\n",
        "      for _, anno in annos.items():\r\n",
        "        anno = anno[\"shape_attributes\"]\r\n",
        "        px = anno[\"points_x\"]\r\n",
        "        py = anno[\"points_y\"]\r\n",
        "        poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\r\n",
        "        poly = [p for x in poly for p in x]\r\n",
        "\r\n",
        "        obj = {\r\n",
        "            \"bbox\": [np.min(px), np.min(py), np.max(px), np.max(py)],\r\n",
        "            \"bbox_mode\": BoxMode.XYXY_ABS,\r\n",
        "            \"segmentation\": [poly],\r\n",
        "            \"category_id\": num - 1,\r\n",
        "        }\r\n",
        "        objs.append(obj)\r\n",
        "      record[\"annotations\"] = objs\r\n",
        "      character_dicts.append(record)\r\n",
        "\r\n",
        "    dataset_dicts.extend(character_dicts)\r\n",
        "  return dataset_dicts\r\n",
        "  \r\n",
        "\r\n",
        "for d in [\"train\", \"test\"]:\r\n",
        "  DatasetCatalog.register(\"character_\" + d, lambda d=d: get_character_dict(\"English Alphabet Dataset/\" + d + \"/\"))\r\n",
        "  MetadataCatalog.get(\"character_\" + str(d)).set(thing_classes=[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\"])\r\n",
        "character_metadata = MetadataCatalog.get(\"character_train\")\r\n",
        "print(\"Finished Successfully\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-ce7fe43f6685>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m   \u001b[0mDatasetCatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"character_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_character_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"English Alphabet Dataset/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m   \u001b[0mMetadataCatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"character_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthing_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"2\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"3\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"4\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"5\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"6\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"7\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mcharacter_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMetadataCatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"character_train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/detectron2/data/catalog.py\u001b[0m in \u001b[0;36mregister\u001b[0;34m(self, name, func)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \"\"\"\n\u001b[1;32m     36\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"You must register a function with `DatasetCatalog.register`!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Dataset '{}' is already registered!\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Dataset 'character_train' is already registered!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKoLgcuBRkjK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "3f1ca953-fd97-41e7-aa4f-6c863637edfa"
      },
      "source": [
        "#Check if the metadata is added successfully\r\n",
        "\r\n",
        "from detectron2.utils.visualizer import Visualizer\r\n",
        "dataset_dicts = get_character_dict(\"English Alphabet Dataset/train/\")\r\n",
        "\r\n",
        "for d in random.sample(dataset_dicts, 3):\r\n",
        "  print(d)\r\n",
        "  img = cv2.imread(d[\"file_name\"])\r\n",
        "  visualizer = Visualizer(img[:, :, ::-1], metadata=character_metadata, scale=0.5)\r\n",
        "  out = visualizer.draw_dataset_dict(d)\r\n",
        "  cv2_imshow(out.get_image()[:, :, ::-1])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'file_name': 'English Alphabet Dataset/train/5/5_29.png', 'image_id': '5_29', 'height': 60, 'width': 60, 'annotations': [{'bbox': [1, 6, 58, 56], 'bbox_mode': <BoxMode.XYXY_ABS: 0>, 'segmentation': [[1.5, 6.5, 58.5, 56.5]], 'category_id': 4}]}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAIAAAC0Ujn1AAAE4ElEQVR4nLVVbWxTZRQ+7/3qx9bbbuu6jrQLmK5kzKxqKAltWAwjIY6ZDFG2ETTyQ2IMhijRIMaQkKg/DPGfCT8MSNSAIwET2dCZIBMG2WbrlA8XtsnK6Me2trdb7+396L2vP1pH166wH/D8ued9z3menHPe9z0XYYzh6YB4SroAgJpcz6qKCqjYceLo94XLbDarqmo+HYIgSYogSjjLQQkpUUhJ7zZ9J+MMBk3D6tn59wEgOpXIKoosywCgYTw9PR0JR3KcWlut0+HQ6/UAwDAMRdMEsUL1lM5IS7yiafgC97FM8gBA6UhV1QTgovFoKBR6GGrOG0kplpyM5eyGhga73W6ttppMJoTydXDRtCxmKUBgNOsQQqk5wbKOZoy0KIpSWrjFXwMTVDQ/umiIw2Scn1zvWO/e3Op2u3Pp/3bqz9l7HJWLQAS8sf54Vs7eXrgUEC5ks9nHSC7H+Ph4IpHw+/0+n48kyXyVuc/5hY8wy2cW8U72swfK3QdoLLdPEERTU5PT6TQYDIVaiqLE4/Hh4eGls52bm+vv74/FYm1tbcukeS2hUylOmb0r/W4nNoQW/8IYrNaa9vb2NWvWVFVV0TRdKK2qKs/zLperr68vHo/nNiVJun79+saNG3NvhQAACulAoZLJJI306wzeBDWpSSjxs17PGDwej81mK9IFAJIkWZb1eDytra319fWFrt7e3nA4ks/aSFh2VB/RsEYg8rbw67/KMFUNlYw5ddGS2S8bWV25FiOENm3aND09HYlEljaj0ai10kUjIwUAC2rs27kDopYu4ID/Ayd/jT265eThi3tqHGw5dYZhls6tCBQAYIwxFE+S0T9GsQ4WzcyBDcetnQJTq5VTL4UsyzoySwCAJEmCIKxUL7Be2bJFnDtnFEMrp7YigsHg1OQkunwyePWX4ZnEVEWzUi5UnCHjPxksrVLFhrIxheBv0Y7qZwgAgMfNVb1Dtb0mpIZ0qRvMamcwxqsdqnSNVtfNZybo5IAeq6uiUADQ2NhYj623haFCh9fr7e7uLooWj8tfdp3D4/i9H141mMpeytwMIQAAEcSKU7EU+krmwx+7a5zs0dZTifDi47NeEel0OhKJFL00ACApYv+JjvOfX/3E9/Xhi3uczbb5+fl0Ol0YIwgC1rS8tNFodNldExMTS+47d+6kUql9+/ZZLJaimhBCrxzZYm0wH9t6+uCZXX9Hb4yOjhYGNNJeE1mT59TV1XV0dBQlyHFcf3//4OBgIpEoLcvf0/z6V1u/2HXmZt/9IpdOp6Np+mFDzGbz9u3bBwcHM5nMUl2BQCAQCIRCIavVWsRXVTWWjNXu4qfOkpUexuSVcz8Zn89H3q+SuYJesyy7bds2u90+MDAQDocLVcbGxkqz/p8Gtm40f8GYXSAcL5PtO15qaWkZ6b07y3HLmkgQREtLS1dXl8fjKatVAsqEbbt5WjKioQZP8/MVFRV5tdJQh8PR2dl56NChnp6e1Uj7/f53Dr796ZX99rW1x7ae5mL524IunwzO3uNsay0vvvlcEUdVVVEUc/bMzMzIyEgwGMwt3W733r17czbDMBRFIYQwxueOXbnyzVjbWy8okpqXZvSUxV65+iY8Av9cDQ2dvbl5d3P+GGUxO3uPeyLS1Q7Wu7NJV8k8sWQLYVtrAYD/ACPHB+SmG1U6AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=30x30 at 0x7F6CAC49E828>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{'file_name': 'English Alphabet Dataset/train/6/6_10.png', 'image_id': '6_10', 'height': 60, 'width': 60, 'annotations': [{'bbox': [2, 5, 58, 57], 'bbox_mode': <BoxMode.XYXY_ABS: 0>, 'segmentation': [[2.5, 5.5, 58.5, 57.5]], 'category_id': 5}]}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAIAAAC0Ujn1AAAEBElEQVR4nLWWTUwTaRjHn7cdh2HasYulhWYra9ZCaRvZxU1YGr8i6QGQRJNGg8SEmOUAJ/FC4gXEg5647B5ckt0gQQPScCAbtbtmO1UjQmMCSkLpbvlqbYFuF7BTWtqZzuyh2EC7fBjr//Q+7/95f/O8nxkkCAJ8HmETE+61tfDnQKOSEt36egzHMYS2GXfudCcbPJ+Ix+OhEBMOM6EQE41GV1dXV1b+XV4OhMObNalUKo1GI5MdVCiUcvmh3NxckUiEAQDHcfE4W1ioOH++SalUC4IwPPyLXE7F46zP987lck1OTno8nkgkKpGQSqWyuLi4rEzHccULCwuvXo0CgNu95nY7AYAkSZVKZTB8o9VqMQCQSIiNjbjJdNnjcT558qtIJD5wAD98mOrr67NYLC6XKzUVvV5/+rSpra2NJEmE0NDQ0MDAz2nr4HTCzMxMXd0FLBnL5YeOHjU8ePCjSiUXi4VAYLmlpcVut/M8v3WYSCQiCEIikSRDmUxWVlY2NzfHMEwqhyAIhSKfog5uovPyFLHYemPjdYVCvbQ039fXZbPZ9tyoysrKwcFBmqaHh4etVisAVFdXm81mgigUBAz7UI64oKDIZhuYnp6sqqo3mS719/+0J1oqlWq1Wq1W29zcvLXfbp8IBNZEySAUWmGY1aWl+ZwcbGzs6ZEj+vz8rzJZKO0Y7apNdDj8nmFW8/IKAKC09Fuv9y+j8ZJKVbJ/UKawVMtmG6it/QEh5PPN373bgePUyZMNb9784fG8TeV83NUtKdEVFqpxPDfToijFuXPXNZqKncaazWbh/0TT4w8f0qJdvsow/9B0j0bzvV5/5iOK/SBE0+OLi0GKIoxGHQA4nc6Ghgav15vKyMmRnDp1JRhcmJj4XacrNZvNra2tSQvHcYqiMqHJE4IBgFiMSaVSuVwOADKZTCwWb82LxdafPes9ceJyRcWFSMRNkmQyc0/ttiApsezGixf3cZxUq408v9+d3BcaABIJ9uXLAZ7nZma4SCSWTTQACELC73cQBOrs7A2F1rOJTkqtFpeXa9rbe4LB99lEI4QQQvX1VSbTd+3tPX5/MGvolOrqjBcvnrl5s3d2djE76K0X/ezZ8qam2tu3709NzWcBnaaKCt21a+auLsvr165M95PQAHDs2Nc3bjR0d//2/PnbNOtT0QCg0XzZ0dHY3//n48djO6ITiQTLsru8nIIgcByXmaNWKzo7r1qtDovFnrK2oWdnZ0dGRqLR6E7ocDjsdrsdDgfLsmmWUvnFrVtXHY7pe/esSTqi6fFAYI3nOZvtqd/v223m21VTU2MwGNI6YzH20aNRgsCPHy/eRO+fuKc4LjE6OlVUVIDGx//O+j8fz/MIof8AsrLJEDOcCp8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=30x30 at 0x7F6CAC3E9128>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{'file_name': 'English Alphabet Dataset/train/6/6_23.png', 'image_id': '6_23', 'height': 60, 'width': 60, 'annotations': [{'bbox': [2, 5, 58, 57], 'bbox_mode': <BoxMode.XYXY_ABS: 0>, 'segmentation': [[2.5, 5.5, 58.5, 57.5]], 'category_id': 5}]}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAB4AAAAeCAIAAAC0Ujn1AAAEOElEQVR4nLWWX0xTVxzHv6f33NtLKYVaJgUKLX/WBSwaykbThAbRGH0wMSQ88GLUBzFmiQ/GZdmfQHzYyxISt0RfNc7EhIwtmSE2adApZoHEPZWNkGG9VBzW20ml3P65be/ZA9BVrAiEfZ/Ozfn+Pr/f/d1zzzmEMYb/R9Tvf/HiRWq7YZqmpVKpbDa7+qjT6URRpJQWeojD0RaLqaLIEfJG8MDASJ6SzWZjsVg0GpVlWVES4fB8JBJ5O19lZWVTU7PdXl9TU2MwGChjLJXSEolcdfUHZWWXKW1mjCnKoMNRqmna8vKyJElTU1NPn4aK4goVjb6MRv9UVTdjH9tsDZQQYjbzKytZSj+jdJLjvgBoebl46pQ9mUxevTo6NfVTMBhUFCWPcLlcHR0dXV1dLS0ttbW1mqYlk8lIJBIKhaanp+vrbQsLRo7j1rpjsewxGj+Zmfnc6TQKQhZYefbsWTAY9Pv9k5OTqx6O4xobG81m87lz5/r6+gwGQ2Fz9+3bd+jQodXxjRuSJCl0PcxG6eumpm9zOWc2O0vpsCzL09PTsVgsHywIQm9vr8fjaW9vN5lMmzcHgG59wAEflZT8nEj0Ly0tZzKnJEkKBAKLi4uFbp7nBUHgOO693P/QmhYBXgJ/WCyCKN5PJpvHxxfHx8c3oLcluo6OMhZhrA6YNxq9qjp/86YF6AB+L3SrqppKpRRFicfjb7N4ntfr9WR9FROHo01R0slkvKSkae/e7wnhMxkpEvnUZGoOh/uBH4CJVSvHcXa7vaKioqysTBTFt9GdnZ0nT560Wq2jo/+sfUZN09Lp9MrKI1l2532xWBiYBb4CSgE/gFwuFwqFir47IUSn0zHGvF5vPisFQClvsVhE0ZROp/Nul8sVCASAIeBrwAj8yPO8x+Ox2WxVVVUbVoggCFar1el0ut1uURSBBQB0aOgXSVIcjtLTpx2F7uHh4UAgAMjAIPAlYOS4ke7ubq/X29bWVl9fX7T8Qune6wBeA5eBhkzmbC6Hre+UW0EDSADfMGYcHXWo6hbJW0UDUAXhO57XhobS8Xh2d9EgJHfiRLiujvT3z8hy+r3+baAB6HQ4f57v6anw+e6Hw4ndRAMghFy6VDcw0Ojz3Z+dLfJP5kU3mdtEFy86zWb+4MFfx8a63G5zUc+2q87rzJmGa9fcx45NPHgg7zIaQG9v7e3bnr6+3+7c+XuX0QAOH64aG/OdPfv41q35DVM77HWhOjv33Lt38OjRh69eqRcufLhzNNlwqQAAtLaaJiZ6jhx5uLSkDg627hD9rj3E4Sh99KhntfYDByoAkOvXn0qSIoqc1frG7v78+fO5ubknT57IsqxpGsdxer3e5/PZ7Xaj0ajX64smSCSyV678VVpKjx+vXkO/q0ZN0xhjjDFCyBZP20xGGxlZ2L+/nNy9u7iDO9/myuWYTod/AeJ0t81lk5QhAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=30x30 at 0x7F6CAC3A76A0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rT9FRG6xN6eK"
      },
      "source": [
        "**Train the Model**\r\n",
        "\r\n",
        "Want to fine-tune a COCO-pretrained R50-FPN 3x with model reference number of 137849458 for Faster RCNN\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LNLzfkUN830"
      },
      "source": [
        "from detectron2.engine import DefaultTrainer\r\n",
        "from detectron2 import model_zoo\r\n",
        "\r\n",
        "cfg = get_cfg()\r\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\r\n",
        "#cfg.DATASETS.TRAIN = (\"character_train_1\",\"character_train_2\", \"character_train_3\", \"character_train_4\", \"character_train_5\",\"character_train_6\",\"character_train_7\",)\r\n",
        "cfg.DATASETS.TRAIN = (\"character_train\",)\r\n",
        "cfg.DATASETS.TEST = ()\r\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\r\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\r\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\r\n",
        "cfg.SOLVER.BASE_LR = 0.0002\r\n",
        "cfg.SOLVER.MAX_ITER = 100\r\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512\r\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 7\r\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93SXm0dESNMx",
        "outputId": "b6da652b-65c3-42e8-f2ab-8e783a3f7ee1"
      },
      "source": [
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\r\n",
        "trainer = DefaultTrainer(cfg) \r\n",
        "trainer.resume_or_load(resume=False)\r\n",
        "trainer.train()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[01/19 14:17:16 d2.engine.defaults]: \u001b[0mModel:\n",
            "GeneralizedRCNN(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelMaxPool()\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (proposal_generator): RPN(\n",
            "    (rpn_head): StandardRPNHead(\n",
            "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (anchor_generator): DefaultAnchorGenerator(\n",
            "      (cell_anchors): BufferList()\n",
            "    )\n",
            "  )\n",
            "  (roi_heads): StandardROIHeads(\n",
            "    (box_pooler): ROIPooler(\n",
            "      (level_poolers): ModuleList(\n",
            "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
            "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
            "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
            "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
            "      )\n",
            "    )\n",
            "    (box_head): FastRCNNConvFCHead(\n",
            "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
            "      (fc_relu1): ReLU()\n",
            "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "      (fc_relu2): ReLU()\n",
            "    )\n",
            "    (box_predictor): FastRCNNOutputLayers(\n",
            "      (cls_score): Linear(in_features=1024, out_features=8, bias=True)\n",
            "      (bbox_pred): Linear(in_features=1024, out_features=28, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m[01/19 14:17:16 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 210 images left.\n",
            "\u001b[32m[01/19 14:17:16 d2.data.build]: \u001b[0mDistribution of instances among all 7 categories:\n",
            "\u001b[36m|  category  | #instances   | category   | #instances   | category   | #instances   |\n",
            "|:----------:|:-------------|:-----------|:-------------|:-----------|:-------------|\n",
            "|     1      | 30           | 2          | 30           | 3          | 30           |\n",
            "|     4      | 30           | 5          | 30           | 6          | 30           |\n",
            "|     7      | 30           |            |              |            |              |\n",
            "|   total    | 210          |            |              |            |              |\u001b[0m\n",
            "\u001b[32m[01/19 14:17:16 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
            "\u001b[32m[01/19 14:17:16 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
            "\u001b[32m[01/19 14:17:16 d2.data.common]: \u001b[0mSerializing 210 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[01/19 14:17:16 d2.data.common]: \u001b[0mSerialized dataset takes 0.09 MiB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (8, 1024) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (8,) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (28, 1024) in the model! You might want to double check if this is expected.\n",
            "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (28,) in the model! You might want to double check if this is expected.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[01/19 14:17:19 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/detectron2/modeling/roi_heads/fast_rcnn.py:217: UserWarning: This overload of nonzero is deprecated:\n",
            "\tnonzero()\n",
            "Consider using one of the following signatures instead:\n",
            "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
            "  num_fg = fg_inds.nonzero().numel()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[01/19 14:17:38 d2.utils.events]: \u001b[0m eta: 0:01:18  iter: 19  total_loss: 2.022  loss_cls: 1.908  loss_box_reg: 0.09536  loss_rpn_cls: 0.002849  loss_rpn_loc: 0.008905  time: 0.9698  data_time: 0.0138  lr: 3.9962e-06  max_mem: 1826M\n",
            "\u001b[32m[01/19 14:17:57 d2.utils.events]: \u001b[0m eta: 0:00:58  iter: 39  total_loss: 1.782  loss_cls: 1.666  loss_box_reg: 0.09348  loss_rpn_cls: 0.004034  loss_rpn_loc: 0.007398  time: 0.9689  data_time: 0.0067  lr: 7.9922e-06  max_mem: 1826M\n",
            "\u001b[32m[01/19 14:18:17 d2.utils.events]: \u001b[0m eta: 0:00:39  iter: 59  total_loss: 1.283  loss_cls: 1.185  loss_box_reg: 0.09445  loss_rpn_cls: 0.002994  loss_rpn_loc: 0.007994  time: 0.9762  data_time: 0.0063  lr: 1.1988e-05  max_mem: 1826M\n",
            "\u001b[32m[01/19 14:18:36 d2.utils.events]: \u001b[0m eta: 0:00:19  iter: 79  total_loss: 0.7841  loss_cls: 0.6463  loss_box_reg: 0.09999  loss_rpn_cls: 0.004251  loss_rpn_loc: 0.006641  time: 0.9728  data_time: 0.0071  lr: 1.5984e-05  max_mem: 1826M\n",
            "\u001b[32m[01/19 14:18:57 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 99  total_loss: 0.427  loss_cls: 0.2964  loss_box_reg: 0.09198  loss_rpn_cls: 0.005169  loss_rpn_loc: 0.007561  time: 0.9719  data_time: 0.0065  lr: 1.998e-05  max_mem: 1826M\n",
            "\u001b[32m[01/19 14:18:58 d2.engine.hooks]: \u001b[0mOverall training speed: 98 iterations in 0:01:35 (0.9719 s / it)\n",
            "\u001b[32m[01/19 14:18:58 d2.engine.hooks]: \u001b[0mTotal training time: 0:01:37 (0:00:02 on hooks)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_JnwjxcSj7y"
      },
      "source": [
        "**Evaluate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXnDioJTSlpO"
      },
      "source": [
        "cfg.MODEL.WEIGHTS = os.path.join(\"output/model_final.pth\")  # path to the model we just trained\r\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.3\r\n",
        "predictor = DefaultPredictor(cfg)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5iAjQQr4u22"
      },
      "source": [
        "from google.colab.patches import cv2_imshow\r\n",
        "\r\n",
        "def get_character_dict_test(img_dir):\r\n",
        "  dataset_dicts = []\r\n",
        "  for num in range(1,8):\r\n",
        "    dir = img_dir + str(num)+ \"/\" + str(num) + \".json\"\r\n",
        "    json_file = os.path.join(dir)\r\n",
        "  \r\n",
        "    with open(json_file) as f:\r\n",
        "      imgs_anns = json.load(f)\r\n",
        "  \r\n",
        "    character_dicts = []\r\n",
        "    for idx, v in zip(imgs_anns.keys(), imgs_anns.values()):\r\n",
        "      record = {}\r\n",
        "\r\n",
        "      filename = os.path.join(img_dir + str(num) + \"/\" + v[\"filename\"])\r\n",
        "      img = cv2.imread(filename)\r\n",
        "      height, width = img.shape[:2]\r\n",
        "\r\n",
        "      record[\"file_name\"] = filename\r\n",
        "      record[\"image_id\"] = idx\r\n",
        "      record[\"height\"] = height\r\n",
        "      record[\"width\"] = width\r\n",
        "\r\n",
        "      annos = v[\"regions\"]\r\n",
        "      objs = []\r\n",
        "      for _, anno in annos.items():\r\n",
        "        anno = anno[\"shape_attributes\"]\r\n",
        "        px = anno[\"points_x\"]\r\n",
        "        py = anno[\"points_y\"]\r\n",
        "        poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\r\n",
        "        poly = [p for x in poly for p in x]\r\n",
        "\r\n",
        "        obj = {\r\n",
        "            \"bbox\": [np.min(px), np.min(py), np.max(px), np.max(py)],\r\n",
        "            \"bbox_mode\": BoxMode.XYXY_ABS,\r\n",
        "            \"segmentation\": [poly],\r\n",
        "            \"category_id\": num - 1,\r\n",
        "        }\r\n",
        "        objs.append(obj)\r\n",
        "      record[\"annotations\"] = objs\r\n",
        "      character_dicts.append(record)\r\n",
        "\r\n",
        "    dataset_dicts.extend(character_dicts)\r\n",
        "  return dataset_dicts"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "HdpKYb42qzk1",
        "outputId": "625dc13a-f049-4717-e1d3-7f4bac081794"
      },
      "source": [
        "from detectron2.utils.visualizer import ColorMode\r\n",
        "\r\n",
        "dataset_dicts = get_character_dict_test(\"English Alphabet Dataset/train/\")\r\n",
        "for d in random.sample(dataset_dicts, 3):  \r\n",
        "#for d in dataset_dicts:  \r\n",
        "    im = cv2.imread(d[\"file_name\"])\r\n",
        "    outputs = predictor(im)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\r\n",
        "    print(outputs)\r\n",
        "    v = Visualizer(im[:, :, ::-1],\r\n",
        "                   metadata=character_metadata, \r\n",
        "                   scale=1,\r\n",
        "                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\r\n",
        "    )\r\n",
        "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\r\n",
        "    cv2_imshow(out.get_image()[:, :, ::-1])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'instances': Instances(num_instances=0, image_height=60, image_width=60, fields=[pred_boxes: Boxes(tensor([], device='cuda:0', size=(0, 4))), scores: tensor([], device='cuda:0'), pred_classes: tensor([], device='cuda:0', dtype=torch.int64)])}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=60x60 at 0x7F6C68AC3F60>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{'instances': Instances(num_instances=0, image_height=60, image_width=60, fields=[pred_boxes: Boxes(tensor([], device='cuda:0', size=(0, 4))), scores: tensor([], device='cuda:0'), pred_classes: tensor([], device='cuda:0', dtype=torch.int64)])}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=60x60 at 0x7F6C68A72B00>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{'instances': Instances(num_instances=0, image_height=60, image_width=60, fields=[pred_boxes: Boxes(tensor([], device='cuda:0', size=(0, 4))), scores: tensor([], device='cuda:0'), pred_classes: tensor([], device='cuda:0', dtype=torch.int64)])}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=60x60 at 0x7F6C68A2E630>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}